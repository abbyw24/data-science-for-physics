{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of training set and validation set are:\n",
      "dict_keys(['bp_coef', 'rp_coef', 'labels']) \n",
      " dict_keys(['bp_coef', 'rp_coef', 'labels'])\n",
      "array shape of traing set and validation set are:\n",
      "[(4000, 55), (4000, 55), (4000, 2)] \n",
      " [(1000, 55), (1000, 55), (1000, 2)]\n"
     ]
    }
   ],
   "source": [
    "# load in data from Problem Set 5\n",
    "# load in data from pset5\n",
    "tr_file  = \"ap17_xpcont_train.pickle\"\n",
    "val_file = \"ap17_xpcont_validation.pickle\"\n",
    "\n",
    "with open(tr_file, 'rb') as f1:\n",
    "    data_tr = pickle.load(f1)\n",
    "\n",
    "with open(val_file, 'rb') as f2:\n",
    "    data_val = pickle.load(f2)\n",
    "    \n",
    "print(\"Keys of training set and validation set are:\")\n",
    "print(data_tr.keys(), \"\\n\", data_val.keys())\n",
    "\n",
    "print(\"array shape of traing set and validation set are:\")\n",
    "print([data_tr[_].shape for _ in data_tr.keys()], \"\\n\", [data_val[_].shape for _ in data_val.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_tr, rp_tr = data_tr['bp_coef'], data_tr['rp_coef']\n",
    "teff_tr = data_tr['labels'][:,0]\n",
    "\n",
    "nstars = len(teff_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = effective temperature\n",
    "Teffs = data_tr['labels'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 9) (4000, 9)\n"
     ]
    }
   ],
   "source": [
    "# construct our features, using the first 8-9 coefficients from BPs and RPs, and dividing out by the first RP term\n",
    "BPs = np.zeros((nstars, 8))\n",
    "RPs = np.zeros((nstars, 9))\n",
    "for idx in range(nstars):\n",
    "    BPs[idx] = data_tr['bp_coef'][idx,:8] / data_tr['rp_coef'][idx,0]\n",
    "    RPs[idx] = data_tr['rp_coef'][idx,:9] / data_tr['rp_coef'][idx,0]\n",
    "BPs = np.insert(BPs, 0, 1., axis=1)\n",
    "print(BPs.shape, RPs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try to use pytorch to implement a neural network to learn the temperature labels from the BP and RP spectral coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bp = torch.HalfTensor(BPs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now the notebook kernel restarts automatically after trying to create a PyTorch tensor with the BP/RP coefficients as in the above cellâ€”I think it's a memory allocation issue (?). I'm going ahead and submitting what I have but will continue to try to figure this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaller, example linear regression with pytorch to make sure I understand the process:\n",
    "\n",
    "This is copied almost directly from https://www.docker.com/blog/how-to-train-and-deploy-a-linear-regression-model-using-pytorch-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fake data set\n",
    "def synthetic_data(m, c, num_examples):\n",
    "    \"\"\"Generate y = mX + bias(c) + noise\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(m)))\n",
    "    y = torch.matmul(X, m) + c\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_m = torch.tensor([2, -3.4])\n",
    "true_c = 4.2\n",
    "features, labels = synthetic_data(true_m, true_c, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 2]), torch.Size([1000, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data set and create a small batch\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)  # TensorDataset() stores the samples and their corresponding labels\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)   # DataLoader() wraps an iterator around the TensorDataset for easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.7861, -0.6197],\n",
       "         [-0.9774,  0.3925],\n",
       "         [-0.3230,  1.0095],\n",
       "         [-0.2083,  0.6822],\n",
       "         [-3.6081, -0.4284],\n",
       "         [ 0.5964, -0.1668],\n",
       "         [ 3.7271,  0.3294],\n",
       "         [-1.7750, -0.3569],\n",
       "         [-1.4828, -0.7950],\n",
       "         [-0.2975, -1.2618]]),\n",
       " tensor([[ 7.8860],\n",
       "         [ 0.9126],\n",
       "         [ 0.1178],\n",
       "         [ 1.4621],\n",
       "         [-1.5677],\n",
       "         [ 5.9650],\n",
       "         [10.5411],\n",
       "         [ 1.8576],\n",
       "         [ 3.9408],\n",
       "         [ 7.8888]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "next(iter(data_iter))       # iter() creates a Python iterator, while next() obtains the first item from that iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a model: initialization\n",
    "# create a single-layer feed-forward network with 2 inputs and 1 output\n",
    "net = nn.Linear(2, 1)\n",
    "# initialize model params\n",
    "net.weight.data.normal_(0, 0.01)    # initial weights are Gaussian normal distribution with mu=0, sigma=0.01\n",
    "net.bias.data.fill_(0)              # bias is set simply to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function: RMS\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an optimization algorithm: stochastic gradient descent\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)            # lr = learning rate; this determines the update step during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING\n",
    "\n",
    "For each minibatch:\n",
    "- Compute predictions and calculate the loss\n",
    "- Calculate gradients by running the backpropagation\n",
    "- Update the model parameters\n",
    "- Compute the loss after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 28.256584\n",
      "epoch 1, loss 23.767817\n",
      "epoch 1, loss 21.012089\n",
      "epoch 1, loss 19.068922\n",
      "epoch 1, loss 17.176157\n",
      "epoch 1, loss 15.164724\n",
      "epoch 1, loss 12.978922\n",
      "epoch 1, loss 11.124973\n",
      "epoch 1, loss 8.468372\n",
      "epoch 1, loss 7.400248\n",
      "epoch 1, loss 6.494429\n",
      "epoch 1, loss 6.079139\n",
      "epoch 1, loss 5.574371\n",
      "epoch 1, loss 4.859051\n",
      "epoch 1, loss 4.308041\n",
      "epoch 1, loss 3.731048\n",
      "epoch 1, loss 3.390696\n",
      "epoch 1, loss 3.044938\n",
      "epoch 1, loss 2.736085\n",
      "epoch 1, loss 2.508322\n",
      "epoch 1, loss 2.318806\n",
      "epoch 1, loss 2.107263\n",
      "epoch 1, loss 1.941071\n",
      "epoch 1, loss 1.748525\n",
      "epoch 1, loss 1.498306\n",
      "epoch 1, loss 1.416942\n",
      "epoch 1, loss 1.060823\n",
      "epoch 1, loss 0.923746\n",
      "epoch 1, loss 0.813718\n",
      "epoch 1, loss 0.747465\n",
      "epoch 1, loss 0.678934\n",
      "epoch 1, loss 0.608004\n",
      "epoch 1, loss 0.499118\n",
      "epoch 1, loss 0.402225\n",
      "epoch 1, loss 0.377671\n",
      "epoch 1, loss 0.326749\n",
      "epoch 1, loss 0.286112\n",
      "epoch 1, loss 0.243903\n",
      "epoch 1, loss 0.205048\n",
      "epoch 1, loss 0.188293\n",
      "epoch 1, loss 0.150663\n",
      "epoch 1, loss 0.131049\n",
      "epoch 1, loss 0.112258\n",
      "epoch 1, loss 0.095482\n",
      "epoch 1, loss 0.091424\n",
      "epoch 1, loss 0.081671\n",
      "epoch 1, loss 0.073053\n",
      "epoch 1, loss 0.065990\n",
      "epoch 1, loss 0.059079\n",
      "epoch 1, loss 0.051128\n",
      "epoch 1, loss 0.044739\n",
      "epoch 1, loss 0.037974\n",
      "epoch 1, loss 0.034714\n",
      "epoch 1, loss 0.030800\n",
      "epoch 1, loss 0.027253\n",
      "epoch 1, loss 0.023231\n",
      "epoch 1, loss 0.018351\n",
      "epoch 1, loss 0.015331\n",
      "epoch 1, loss 0.013223\n",
      "epoch 1, loss 0.011579\n",
      "epoch 1, loss 0.010438\n",
      "epoch 1, loss 0.009291\n",
      "epoch 1, loss 0.008316\n",
      "epoch 1, loss 0.007352\n",
      "epoch 1, loss 0.006138\n",
      "epoch 1, loss 0.005059\n",
      "epoch 1, loss 0.004285\n",
      "epoch 1, loss 0.003668\n",
      "epoch 1, loss 0.003315\n",
      "epoch 1, loss 0.002914\n",
      "epoch 1, loss 0.002492\n",
      "epoch 1, loss 0.002296\n",
      "epoch 1, loss 0.002030\n",
      "epoch 1, loss 0.001828\n",
      "epoch 1, loss 0.001597\n",
      "epoch 1, loss 0.001428\n",
      "epoch 1, loss 0.001362\n",
      "epoch 1, loss 0.001296\n",
      "epoch 1, loss 0.001152\n",
      "epoch 1, loss 0.001017\n",
      "epoch 1, loss 0.000927\n",
      "epoch 1, loss 0.000810\n",
      "epoch 1, loss 0.000718\n",
      "epoch 1, loss 0.000647\n",
      "epoch 1, loss 0.000543\n",
      "epoch 1, loss 0.000502\n",
      "epoch 1, loss 0.000461\n",
      "epoch 1, loss 0.000421\n",
      "epoch 1, loss 0.000372\n",
      "epoch 1, loss 0.000353\n",
      "epoch 1, loss 0.000320\n",
      "epoch 1, loss 0.000308\n",
      "epoch 1, loss 0.000285\n",
      "epoch 1, loss 0.000267\n",
      "epoch 1, loss 0.000229\n",
      "epoch 1, loss 0.000228\n",
      "epoch 1, loss 0.000198\n",
      "epoch 1, loss 0.000186\n",
      "epoch 1, loss 0.000182\n",
      "epoch 1, loss 0.000168\n",
      "epoch 2, loss 0.000159\n",
      "epoch 2, loss 0.000159\n",
      "epoch 2, loss 0.000150\n",
      "epoch 2, loss 0.000148\n",
      "epoch 2, loss 0.000144\n",
      "epoch 2, loss 0.000141\n",
      "epoch 2, loss 0.000133\n",
      "epoch 2, loss 0.000129\n",
      "epoch 2, loss 0.000127\n",
      "epoch 2, loss 0.000123\n",
      "epoch 2, loss 0.000122\n",
      "epoch 2, loss 0.000119\n",
      "epoch 2, loss 0.000120\n",
      "epoch 2, loss 0.000116\n",
      "epoch 2, loss 0.000117\n",
      "epoch 2, loss 0.000113\n",
      "epoch 2, loss 0.000111\n",
      "epoch 2, loss 0.000107\n",
      "epoch 2, loss 0.000103\n",
      "epoch 2, loss 0.000103\n",
      "epoch 2, loss 0.000104\n",
      "epoch 2, loss 0.000102\n",
      "epoch 2, loss 0.000101\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000098\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000100\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 2, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000098\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000099\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000100\n",
      "epoch 3, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000100\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000099\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 4, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000099\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n",
      "epoch 5, loss 0.000098\n"
     ]
    }
   ],
   "source": [
    "nepochs = 5\n",
    "for epoch in range(nepochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)\n",
    "        trainer.zero_grad()     # sets gradients to zero\n",
    "        l.backward()            # backpropagation\n",
    "        trainer.step()          # parameter update\n",
    "        l = loss(net(features), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating m: tensor([ 0.0008, -0.0004])\n",
      "error in estimating c: tensor([-1.5736e-05])\n"
     ]
    }
   ],
   "source": [
    "# results: compute errors by comparing the true value with the trained model parameters\n",
    "m = net.weight.data\n",
    "print('error in estimating m:', true_m - m.reshape(true_m.shape))\n",
    "c = net.bias.data\n",
    "print('error in estimating c:', true_c - c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a3e6bf5341c9ae3495e016750e059e6afad886c5c8a5c53cff485cf2c98b0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('comp-phys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
